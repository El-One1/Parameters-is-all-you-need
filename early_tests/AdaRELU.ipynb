{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time as time\n",
    "from gradient_descent_the_ultimate_optimizer import gdtuo\n",
    "from gradient_descent_the_ultimate_optimizer.gdtuo import Optimizable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaRELU(Optimizable):\n",
    "\n",
    "    def __init__(self, positive_slope, negative_slope, optimizer):\n",
    "\n",
    "        self.parameters = {'positive_slope': torch.tensor(positive_slope, requires_grad=True),\n",
    "                           'negative_slope': torch.tensor(negative_slope, requires_grad=True)}\n",
    "        self.optimizer = optimizer\n",
    "        self.all_params_with_gradients = [self.parameters['positive_slope'], self.parameters['negative_slope']]\n",
    "\n",
    "        super().__init__(self.parameters, optimizer)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        output = torch.where(input >= 0, input * self.parameters['positive_slope'], input * self.parameters['negative_slope'])\n",
    "        return output\n",
    "    \n",
    "    def step(self):\n",
    "        self.optimizer.step(self.parameters)\n",
    "\n",
    "adaRELU = AdaRELU(1., 0.01, gdtuo.SGD(alpha = 0.001))\n",
    "adaRELU.initialize()\n",
    "\n",
    "x = torch.tensor([[1., 2., 3.], [7., 12., 8.], [-1., -3., -2000.]], requires_grad=True)\n",
    "y = torch.tensor([[1.5, 3., 4.5], [10.5, 18., 12.], [-.01, -.03, -20.]], requires_grad=True)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    adaRELU.begin()\n",
    "    adaRELU.zero_grad()\n",
    "\n",
    "\n",
    "    y_hat = adaRELU(x)\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss.backward()\n",
    "    adaRELU.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.5000, grad_fn=<SubBackward0>),\n",
       " tensor(0.0100, grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaRELU.parameters['positive_slope'], adaRELU.parameters['negative_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "class MNIST_FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected NN for the MNIST task. This is Optimizable but not itself\n",
    "    an optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inp, num_hid, num_out, adaRELU):\n",
    "        super(MNIST_FullyConnected, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_inp, num_hid)\n",
    "        self.layer2 = nn.Linear(num_hid, num_out)\n",
    "        self.adaRELU = adaRELU\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.kaiming_uniform_(self.layer1.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.layer2.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute a prediction.\"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.adaRELU(x)    ## we want that changed\n",
    "        x = self.layer2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False)\n",
    "\n",
    "adaRELU_MNIST = AdaRELU(1., 0.01, gdtuo.SGD(alpha = 0.001))\n",
    "#optim = gdtuo.SGD(alpha=0.0769)\n",
    "model = MNIST_FullyConnected(28 * 28, 128, 10, adaRELU_MNIST).to(DEVICE)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.000769)\n",
    "\n",
    "#mw = gdtuo.ModuleWrapper(model, optimizer=optim)\n",
    "adaRELU_MNIST.initialize()\n",
    "#mw.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, TRAIN LOSS: 1.0298290298461914, ACC: 0.8464166666666667\n",
      "EPOCH: 2, TRAIN LOSS: 1.0173378355026246, ACC: 0.8492333333333333\n",
      "EPOCH: 3, TRAIN LOSS: 1.017896891816457, ACC: 0.84185\n",
      "Time taken: 12.62617301940918\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "EPOCHS = 3\n",
    "for i in range(1, EPOCHS+1):\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for j, (features_, labels_) in enumerate(dl_train):\n",
    "        #mw.begin() # call this before each step, enables gradient tracking on desired params\n",
    "        adaRELU_MNIST.begin()\n",
    "        features, labels = torch.reshape(features_, (-1, 28 * 28)).to(DEVICE), labels_.to(DEVICE)\n",
    "        pred = model.forward(features)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        #mw.zero_grad()\n",
    "        adaRELU_MNIST.zero_grad()\n",
    "        loss.backward(create_graph=True) # important! use create_graph=True\n",
    "        #mw.step()\n",
    "        optim.step()\n",
    "        adaRELU_MNIST.step()\n",
    "        running_loss += loss.item() * features_.size(0)\n",
    "        running_acc += (torch.argmax(pred, dim=1) == labels).sum().item()\n",
    "    train_loss = running_loss / len(dl_train.dataset)\n",
    "    train_acc = running_acc / len(dl_train.dataset)\n",
    "    print(\"EPOCH: {}, TRAIN LOSS: {}, ACC: {}\".format(i, train_loss, train_acc))\n",
    "print(\"Time taken: {}\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0327, grad_fn=<SubBackward0>),\n",
       " tensor(-0.0065, grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adaRELU.parameters['positive_slope'], model.adaRELU.parameters['negative_slope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "class MNIST_FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected NN for the MNIST task. This is Optimizable but not itself\n",
    "    an optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inp, num_hid, num_out):\n",
    "        super(MNIST_FullyConnected, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_inp, num_hid)\n",
    "        self.layer2 = nn.Linear(num_hid, num_out)\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.kaiming_uniform_(self.layer1.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.layer2.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute a prediction.\"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)    ## we want that changed\n",
    "        x = self.layer2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False)\n",
    "\n",
    "\n",
    "model = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.000769)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, TRAIN LOSS: 1.0661608177185058, ACC: 0.8587833333333333\n",
      "EPOCH: 2, TRAIN LOSS: 1.0500820800145467, ACC: 0.8460166666666666\n",
      "EPOCH: 3, TRAIN LOSS: 1.0420895022074381, ACC: 0.8225166666666667\n",
      "Time taken: 14.543699741363525\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "EPOCHS = 3\n",
    "for i in range(1, EPOCHS+1):\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for j, (features_, labels_) in enumerate(dl_train):\n",
    "        #mw.begin() # call this before each step, enables gradient tracking on desired params\n",
    "        features, labels = torch.reshape(features_, (-1, 28 * 28)).to(DEVICE), labels_.to(DEVICE)\n",
    "        pred = model.forward(features)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        #mw.zero_grad()\n",
    "        loss.backward(create_graph=True) # important! use create_graph=True\n",
    "        #mw.step()\n",
    "        optim.step()\n",
    "        running_loss += loss.item() * features_.size(0)\n",
    "        running_acc += (torch.argmax(pred, dim=1) == labels).sum().item()\n",
    "    train_loss = running_loss / len(dl_train.dataset)\n",
    "    train_acc = running_acc / len(dl_train.dataset)\n",
    "    print(\"EPOCH: {}, TRAIN LOSS: {}, ACC: {}\".format(i, train_loss, train_acc))\n",
    "print(\"Time taken: {}\".format(time.time() - init_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

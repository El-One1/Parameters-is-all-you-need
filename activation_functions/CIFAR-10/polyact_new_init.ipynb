{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time as time\n",
    "import numpy as np\n",
    "from gradient_descent_the_ultimate_optimizer import gdtuo\n",
    "from gradient_descent_the_ultimate_optimizer.gdtuo import Optimizable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Video, Image\n",
    "from poly_fit_relu import train_poly_fit_relu as pfr\n",
    "from poly_fit_relu import plot_poly_fit_relu as ppfr\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self, poly_act):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(12544, 128)  # Adjusted input dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.poly_act = poly_act\n",
    "        self.dict_stats = {}\n",
    "        self.gather_stats = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        if self.gather_stats:\n",
    "            self.dict_stats['conv1_mean'] = x.clone().detach().cpu().numpy().mean()\n",
    "            self.dict_stats['conv1_std'] = x.clone().detach().cpu().numpy().std()\n",
    "        x = self.poly_act(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.gather_stats:\n",
    "            self.dict_stats['conv2_mean'] = x.clone().detach().cpu().numpy().mean()\n",
    "            self.dict_stats['conv2_std'] = x.clone().detach().cpu().numpy().std()\n",
    "        x = self.poly_act(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.gather_stats:\n",
    "            self.dict_stats['fc1_mean'] = x.clone().detach().cpu().numpy().mean()\n",
    "            self.dict_stats['fc1_std'] = x.clone().detach().cpu().numpy().std()\n",
    "        x = self.poly_act(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 8\n",
    "\n",
    "class PolyAct(Optimizable):\n",
    "    def __init__(self, optimizer, ranks = rank, coefs_list = None):\n",
    "        self.n = ranks\n",
    "        self.coefs = torch.randn(np.min([self.n, 4]))\n",
    "        if self.n > 4:\n",
    "            self.coefs = torch.cat((self.coefs, torch.zeros(self.n - 4)))\n",
    "        self.coefs = nn.Parameter(self.coefs)\n",
    "        self.parameters = {'coefs': self.coefs}\n",
    "        if coefs_list is not None:\n",
    "            self.n = len(coefs_list)\n",
    "            self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
    "            self.parameters = {'coefs': self.coefs}\n",
    "                \n",
    "        self.optimizer = optimizer\n",
    "        self.all_params_with_gradients = [self.parameters['coefs']]\n",
    "        super().__init__(self.parameters, self.optimizer)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = 0\n",
    "\n",
    "        for i in range(self.n):\n",
    "            out += self.parameters['coefs'][i] * x ** i\n",
    "        return out\n",
    "    \n",
    "    def step(self):\n",
    "        self.optimizer.step(self.parameters)\n",
    "\n",
    "poly_act = PolyAct(gdtuo.Adam(0.0001), ranks = rank)\n",
    "poly_act_init = PolyAct(gdtuo.Adam(0.001), ranks = rank)\n",
    "poly_act.initialize()\n",
    "\n",
    "model = MNIST_CNN(poly_act).to(DEVICE)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefs so far tensor([-0.2352,  0.5826,  0.9822, -0.2571,  0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefs so far tensor([-2.4279e-01,  5.7846e-01,  9.8638e-01, -2.5771e-01, -3.6745e-04,\n",
      "         1.1149e-04, -9.5481e-04,  2.5590e-05])\n",
      "coefs so far tensor([-2.4939e-01,  5.7664e-01,  9.9008e-01, -2.5804e-01,  2.5385e-04,\n",
      "         2.3853e-04, -9.4190e-04,  6.9518e-05])\n",
      "EPOCH: 1, TRAIN LOSS: 0.310711980565389, ACC: 0.91845\n",
      "{}\n",
      "coefs so far tensor([-2.5187e-01,  5.7634e-01,  9.9110e-01, -2.5826e-01,  4.5258e-04,\n",
      "         2.2792e-04, -9.1979e-04,  6.1382e-05])\n",
      "coefs so far tensor([-2.5796e-01,  5.7594e-01,  9.9424e-01, -2.5871e-01,  1.0286e-03,\n",
      "         2.3197e-04, -8.9454e-04,  6.4794e-05])\n",
      "coefs so far tensor([-2.6465e-01,  5.7636e-01,  9.9692e-01, -2.5895e-01,  1.4801e-03,\n",
      "         2.6491e-04, -8.7818e-04,  7.3570e-05])\n",
      "EPOCH: 2, TRAIN LOSS: 0.11630143987337749, ACC: 0.9711333333333333\n",
      "{}\n",
      "coefs so far tensor([-2.6708e-01,  5.7710e-01,  9.9767e-01, -2.5889e-01,  1.5508e-03,\n",
      "         2.9472e-04, -8.9600e-04,  8.0034e-05])\n",
      "coefs so far tensor([-2.7357e-01,  5.7808e-01,  1.0004e+00, -2.5921e-01,  1.9631e-03,\n",
      "         2.8235e-04, -9.1406e-04,  7.5520e-05])\n",
      "coefs so far tensor([-2.7961e-01,  5.7942e-01,  1.0029e+00, -2.5928e-01,  2.3914e-03,\n",
      "         3.5790e-04, -9.4099e-04,  1.1546e-04])\n",
      "EPOCH: 3, TRAIN LOSS: 0.08384602160851161, ACC: 0.9782833333333333\n",
      "{}\n",
      "coefs so far tensor([-2.8184e-01,  5.7999e-01,  1.0035e+00, -2.5939e-01,  2.4804e-03,\n",
      "         2.9701e-04, -9.4039e-04,  9.2034e-05])\n",
      "coefs so far tensor([-2.8832e-01,  5.8200e-01,  1.0058e+00, -2.5964e-01,  2.8493e-03,\n",
      "         2.5456e-04, -9.4641e-04,  8.4156e-05])\n",
      "coefs so far tensor([-2.9407e-01,  5.8360e-01,  1.0082e+00, -2.5991e-01,  3.1468e-03,\n",
      "         2.2874e-04, -1.0092e-03,  8.0010e-05])\n",
      "EPOCH: 4, TRAIN LOSS: 0.06556635210712751, ACC: 0.9829666666666667\n",
      "{}\n",
      "coefs so far tensor([-2.9610e-01,  5.8419e-01,  1.0090e+00, -2.6006e-01,  3.3507e-03,\n",
      "         1.8857e-04, -9.8035e-04,  7.1128e-05])\n",
      "coefs so far tensor([-3.0195e-01,  5.8592e-01,  1.0118e+00, -2.6027e-01,  3.9169e-03,\n",
      "         1.9364e-04, -9.7921e-04,  8.1219e-05])\n",
      "coefs so far tensor([-3.0803e-01,  5.8810e-01,  1.0136e+00, -2.6048e-01,  4.1814e-03,\n",
      "         1.5365e-04, -1.0255e-03,  7.7511e-05])\n",
      "EPOCH: 5, TRAIN LOSS: 0.05469502836068471, ACC: 0.9857166666666667\n",
      "{}\n",
      "coefs so far tensor([-3.0998e-01,  5.8869e-01,  1.0146e+00, -2.6028e-01,  4.4194e-03,\n",
      "         2.4535e-04, -1.0220e-03,  1.0622e-04])\n",
      "coefs so far tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "coefs so far tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "EPOCH: 6, TRAIN LOSS: nan, ACC: 0.32571666666666665\n",
      "{}\n",
      "coefs so far tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "coefs so far tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(features)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(pred, labels)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m poly_act\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "EPOCHS = 10\n",
    "coefs_list = [poly_act.parameters['coefs'].detach().cpu().numpy()]\n",
    "for i in range(1, EPOCHS+1):\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for j, (features_, labels_) in enumerate(dl_train):\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print('coefs so far', poly_act.parameters['coefs'].detach())\n",
    "            coefs_list.append(poly_act.parameters['coefs'].detach().cpu().numpy())\n",
    "        poly_act.begin()\n",
    "        optim.zero_grad()\n",
    "        poly_act.zero_grad()\n",
    "        features, labels = features_.to(DEVICE), labels_.to(DEVICE)\n",
    "        pred = model.forward(features)\n",
    "        loss = F.nll_loss(pred, labels)\n",
    "        loss.backward(create_graph=True)\n",
    "\n",
    "        optim.step()\n",
    "        poly_act.step()\n",
    "        running_loss += loss.item() * features_.size(0)\n",
    "        running_acc += (torch.argmax(pred, dim=1) == labels).sum().item()\n",
    "    train_loss = running_loss / len(dl_train.dataset)\n",
    "    train_acc = running_acc / len(dl_train.dataset)\n",
    "    print(\"EPOCH: {}, TRAIN LOSS: {}, ACC: {}\".format(i, train_loss, train_acc))\n",
    "    print(model.dict_stats)\n",
    "\n",
    "print(\"Time taken: {}\".format(time.time() - init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/2498081177.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coefs = nn.Parameter(torch.tensor(coefs_list))\n",
      "/tmp/ipykernel_1921728/1292218662.py:21: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  imageio.mimsave(video, [imageio.imread('plots/{}.png'.format(i)) for i in range(len(coefs_list))], fps = 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./polyact.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "for i in range(len(coefs_list)):\n",
    "    curr_poly_act = PolyAct(gdtuo.Adam(0.001), coefs_list = torch.tensor(coefs_list[i]))\n",
    "    y = curr_poly_act(torch.tensor(x)).detach()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    ax.plot(x, np.maximum(x, 0))\n",
    "    # set small cross at 0.0\n",
    "    ax.plot([0.0], [0.0], 'x', color='red')\n",
    "\n",
    "    ax.set_xlim([-4, 4])\n",
    "    ax.set_ylim([-10, 10])\n",
    "    ax.set_yscale('linear')\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    plt.savefig('plots/{}.png'.format(i))\n",
    "    plt.close()\n",
    "\n",
    "video = './polyact.mp4'\n",
    "imageio.mimsave(video, [imageio.imread('plots/{}.png'.format(i)) for i in range(len(coefs_list))], fps = 3)\n",
    "#play it here\n",
    "Video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0436, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mean_weight_CNN = 0\n",
    "std_weight_CNN = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        mean_weight_CNN += param.data.mean()\n",
    "        \n",
    "print(mean_weight_CNN) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
